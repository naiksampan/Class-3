{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99d13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # Optional: for progress display (pip install tqdm)\n",
    "\n",
    "def create_filtered_annotations(\n",
    "    original_json_path,\n",
    "    original_image_dir,\n",
    "    new_image_dir,\n",
    "    output_json_path\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a new COCO-style JSON containing only annotations for images\n",
    "    that exist in the new directory, with updated image paths.\n",
    "    \n",
    "    Args:\n",
    "        original_json_path (str): Path to the original JSON file.\n",
    "        original_image_dir (str): Folder where original images are stored.\n",
    "        new_image_dir (str): Folder containing new filtered images.\n",
    "        output_json_path (str): Output path for new JSON file.\n",
    "    \"\"\"\n",
    "    # Load original annotations\n",
    "    with open(original_json_path, 'r') as f:\n",
    "        original_data = json.load(f)\n",
    "    \n",
    "    # Get available image names (without path) in the new directory\n",
    "    available_images = {os.path.basename(f) for f in os.listdir(new_image_dir)\n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png','.JPG'))}\n",
    "\n",
    "    # Prepare new JSON structure\n",
    "    new_data = {\n",
    "        \"licenses\": original_data.get(\"licenses\", []),\n",
    "        \"info\": original_data.get(\"info\", {}),\n",
    "        \"categories\": original_data.get(\"categories\", []),\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "\n",
    "    # Mapping from old image_id to new image_id\n",
    "    image_id_map = {}\n",
    "    new_image_id = 1\n",
    "    new_annotation_id = 1\n",
    "\n",
    "    # Filter and copy relevant images\n",
    "    for img in tqdm(original_data[\"images\"], desc=\"Processing images\"):\n",
    "        image_name = os.path.basename(img[\"file_name\"])\n",
    "        if image_name in available_images:\n",
    "            # Update file path to new directory\n",
    "            new_img_entry = img.copy()\n",
    "            new_img_entry[\"file_name\"] = os.path.join(\n",
    "                os.path.basename(new_image_dir), image_name\n",
    "            )\n",
    "            new_img_entry[\"id\"] = new_image_id\n",
    "\n",
    "            image_id_map[img[\"id\"]] = new_image_id\n",
    "            new_data[\"images\"].append(new_img_entry)\n",
    "            new_image_id += 1\n",
    "\n",
    "    # Copy annotations for matched images\n",
    "    for ann in tqdm(original_data[\"annotations\"], desc=\"Processing annotations\"):\n",
    "        old_image_id = ann[\"image_id\"]\n",
    "        if old_image_id in image_id_map:\n",
    "            new_ann_entry = ann.copy()\n",
    "            new_ann_entry[\"image_id\"] = image_id_map[old_image_id]\n",
    "            new_ann_entry[\"id\"] = new_annotation_id\n",
    "            new_data[\"annotations\"].append(new_ann_entry)\n",
    "            new_annotation_id += 1\n",
    "\n",
    "    # Save the filtered JSON\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(new_data, f, indent=4)\n",
    "\n",
    "    print(f\"âœ… New annotation file saved: {output_json_path}\")\n",
    "    print(f\"ðŸ“Š Total images included: {len(new_data['images'])}\")\n",
    "    print(f\"ðŸ“Š Total annotations included: {len(new_data['annotations'])}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# create_filtered_annotations(\n",
    "#     original_json_path=\"COMPREESED-2/annotations/person_keypoints_default.json\",\n",
    "#     original_image_dir=\"COMPREESED-2\",\n",
    "#     new_image_dir=\"Dataset/train\",\n",
    "#     output_json_path=\"Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8574ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 70545.20it/s]\n",
      "Processing annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:00<00:00, 95844.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New annotation file saved: Dataset/train/annotation/n-person_keypoints_default.json\n",
      "ðŸ“Š Total images included: 90\n",
      "ðŸ“Š Total annotations included: 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_filtered_annotations(\n",
    "    original_json_path=\"./COMPRESSED 2/annotations/person_keypoints_default.json\",\n",
    "    original_image_dir=\"./COMPRESSED 2\",\n",
    "    new_image_dir=\"./Dataset/train\",\n",
    "    output_json_path=\"Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "590e288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # Optional progress bar\n",
    "\n",
    "def merge_annotations_from_folder(\n",
    "    original_json_path,\n",
    "    new_image_dir,\n",
    "    output_json_path\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge COCO-style annotations from an original JSON into an existing or new JSON,\n",
    "    keeping only images that exist in the specified image directory.\n",
    "    \n",
    "    Args:\n",
    "        original_json_path (str): Path to the original COCO JSON file.\n",
    "        new_image_dir (str): Directory with images to include.\n",
    "        output_json_path (str): Path to the merged output JSON file.\n",
    "    \"\"\"\n",
    "    # Load original JSON\n",
    "    with open(original_json_path, 'r') as f:\n",
    "        orig_data = json.load(f)\n",
    "\n",
    "    # Load or create target JSON\n",
    "    if os.path.exists(output_json_path):\n",
    "        print(f\"ðŸ“‚ Loading existing file: {output_json_path}\")\n",
    "        with open(output_json_path, 'r') as f:\n",
    "            merged_data = json.load(f)\n",
    "    else:\n",
    "        print(f\"ðŸ†• Creating new file: {output_json_path}\")\n",
    "        merged_data = {\n",
    "            \"licenses\": orig_data.get(\"licenses\", []),\n",
    "            \"info\": orig_data.get(\"info\", {}),\n",
    "            \"categories\": orig_data.get(\"categories\", []),\n",
    "            \"images\": [],\n",
    "            \"annotations\": []\n",
    "        }\n",
    "\n",
    "    # Track which image names already exist in merged data\n",
    "    existing_filenames = {os.path.basename(img[\"file_name\"]) for img in merged_data[\"images\"]}\n",
    "\n",
    "    # Get available image names in new folder\n",
    "    available_images = {f for f in os.listdir(new_image_dir)\n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png', '.JPEG', '.JPG'))}\n",
    "\n",
    "    # Next available IDs\n",
    "    next_image_id = max([img[\"id\"] for img in merged_data[\"images\"]], default=0) + 1\n",
    "    next_ann_id = max([ann[\"id\"] for ann in merged_data[\"annotations\"]], default=0) + 1\n",
    "\n",
    "    # Map old to new image IDs\n",
    "    image_id_map = {}\n",
    "\n",
    "    # --- Copy matching images ---\n",
    "    for img in tqdm(orig_data[\"images\"], desc=\"Processing new images\"):\n",
    "        base_name = os.path.basename(img[\"file_name\"])\n",
    "        if base_name in available_images and base_name not in existing_filenames:\n",
    "            new_img = img.copy()\n",
    "            new_img[\"file_name\"] = os.path.join(os.path.basename(new_image_dir), base_name)\n",
    "            new_img[\"id\"] = next_image_id\n",
    "            image_id_map[img[\"id\"]] = next_image_id\n",
    "            merged_data[\"images\"].append(new_img)\n",
    "\n",
    "            existing_filenames.add(base_name)\n",
    "            next_image_id += 1\n",
    "\n",
    "    # --- Copy matching annotations ---\n",
    "    for ann in tqdm(orig_data[\"annotations\"], desc=\"Processing new annotations\"):\n",
    "        old_img_id = ann[\"image_id\"]\n",
    "        if old_img_id in image_id_map:\n",
    "            new_ann = ann.copy()\n",
    "            new_ann[\"image_id\"] = image_id_map[old_img_id]\n",
    "            new_ann[\"id\"] = next_ann_id\n",
    "            merged_data[\"annotations\"].append(new_ann)\n",
    "            next_ann_id += 1\n",
    "\n",
    "    # Save merged JSON\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(merged_data, f, indent=4)\n",
    "\n",
    "    print(f\"âœ… Merged and saved: {output_json_path}\")\n",
    "    print(f\"ðŸ“Š Total images: {len(merged_data['images'])}\")\n",
    "    print(f\"ðŸ“Š Total annotations: {len(merged_data['annotations'])}\")\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# merge_annotations_from_folder(\n",
    "#     original_json_path=\"COMPREESED-2/annotations/person_keypoints_default.json\",\n",
    "#     new_image_dir=\"Dataset/train\",\n",
    "#     output_json_path=\"Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    "# )\n",
    "\n",
    "# To merge another folder later:\n",
    "# merge_annotations_from_folder(\n",
    "#     original_json_path=\"COMPREESED-3/annotations/person_keypoints_default.json\",\n",
    "#     new_image_dir=\"Dataset/train\",\n",
    "#     output_json_path=\"Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "988c72ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ†• Creating new file: Dataset/train/annotation/n-person_keypoints_default.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 32173.88it/s]\n",
      "Processing new annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 10692.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged and saved: Dataset/train/annotation/n-person_keypoints_default.json\n",
      "ðŸ“Š Total images: 11\n",
      "ðŸ“Š Total annotations: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "merge_annotations_from_folder(\n",
    "    original_json_path=\"COMPREESED-2/annotations/person_keypoints_default.json\",\n",
    "    new_image_dir=\"Dataset/train\",\n",
    "    output_json_path=\"Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37a06b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading existing file: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 83165.31it/s]\n",
      "Processing new annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:00<00:00, 161743.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged and saved: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n",
      "ðŸ“Š Total images: 93\n",
      "ðŸ“Š Total annotations: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# To merge another folder later:\n",
    "merge_annotations_from_folder(\n",
    "    original_json_path=\"/Users/nebula/Desktop/Projects/Sanket/COMPRESSED 2/annotations/person_keypoints_default.json\",\n",
    "    new_image_dir=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train\",\n",
    "    output_json_path=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71bd9641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading existing file: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 85580.40it/s]\n",
      "Processing new annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 211531.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged and saved: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n",
      "ðŸ“Š Total images: 138\n",
      "ðŸ“Š Total annotations: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merge_annotations_from_folder(\n",
    "    original_json_path=\"/Users/nebula/Desktop/Projects/Sanket/edited surgical /annotations/person_keypoints_default.json\",\n",
    "    new_image_dir=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train\",\n",
    "    output_json_path=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68b260fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading existing file: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 140230.83it/s]\n",
      "Processing new annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 195721.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged and saved: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n",
      "ðŸ“Š Total images: 188\n",
      "ðŸ“Š Total annotations: 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merge_annotations_from_folder(\n",
    "    original_json_path=\"/Users/nebula/Desktop/Projects/Sanket/task 4/annotations/person_keypoints_default.json\",\n",
    "    new_image_dir=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train\",\n",
    "    output_json_path=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2358144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading existing file: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:00<00:00, 235583.15it/s]\n",
      "Processing new annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:00<00:00, 269917.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged and saved: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n",
      "ðŸ“Š Total images: 199\n",
      "ðŸ“Š Total annotations: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merge_annotations_from_folder(\n",
    "    original_json_path=\"/Users/nebula/Desktop/Projects/Sanket/task 5 /annotations/person_keypoints_default.json\",\n",
    "    new_image_dir=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train\",\n",
    "    output_json_path=\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db17f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Normalized category IDs in: /Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\n",
      "âœ… Normalized category IDs in: /Users/nebula/Desktop/Projects/Sanket/Dataset/validation/annotation/n-person_keypoints_default.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def normalize_yolo_categories(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Mapping for old category IDs\n",
    "    id_map = {1: 0, 18: 1}\n",
    "\n",
    "    # Update annotations\n",
    "    for ann in data[\"annotations\"]:\n",
    "        if ann[\"category_id\"] in id_map:\n",
    "            ann[\"category_id\"] = id_map[ann[\"category_id\"]]\n",
    "\n",
    "    # Update categories section\n",
    "    new_categories = []\n",
    "    for cat in data[\"categories\"]:\n",
    "        if cat[\"id\"] == 1:\n",
    "            cat[\"id\"] = 0\n",
    "            cat[\"name\"] = \"non surgical\"\n",
    "        elif cat[\"id\"] == 18:\n",
    "            cat[\"id\"] = 1\n",
    "            cat[\"name\"] = \"surgical\"\n",
    "        new_categories.append(cat)\n",
    "    data[\"categories\"] = new_categories\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"âœ… Normalized category IDs in: {json_path}\")\n",
    "\n",
    "# Example usage:\n",
    "normalize_yolo_categories(\"/Users/nebula/Desktop/Projects/Sanket/Dataset/train/annotation/n-person_keypoints_default.json\")\n",
    "normalize_yolo_categories(\"/Users/nebula/Desktop/Projects/Sanket/Dataset/validation/annotation/n-person_keypoints_default.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc1cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
